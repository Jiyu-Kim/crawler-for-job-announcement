from requests import get
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

def get_page_count(keyword):
    
    options = Options() # options is an Options' instance
    options.add_experimental_option("detach", True)
    options.add_experimental_option("excludeSwitches", ["enable-logging"])
    browser = webdriver.Chrome(options=options) # Create a browser
    base_url = "https://kr.indeed.com/jobs?q="

    try:
        browser.get(f"{base_url}{keyword}")
    except:
         print("Can't request page")

    soup = BeautifulSoup(browser.page_source, "html.parser")
    pagination = soup.find("nav", attrs={"aria-label":"pagination"})
    pages = pagination.find_all("div", recursive=False) # recursive allows us to tell bs4 to only search in the surface and not go super deep to all the children.
    browser.close()
    count = len(pages)
    if count == 0:
        return 1
    else:
         return count - 1

def extract_indeed_jobs(keyword):
    options = Options() # options is an Options' instance
    options.add_experimental_option("detach", True)
    options.add_experimental_option("excludeSwitches", ["enable-logging"])
    # requests.get automatically sends http request to the web site so that the web site knows requests is generated by bot not by human.
    # selenium actually starts a browser.
    browser = webdriver.Chrome(options=options) # Create a browser

    results = []
    pages = get_page_count(keyword) # Get a number of pages
    for page in range(pages):
        base_url = "https://kr.indeed.com/jobs"
        final_url = f"{base_url}?q={keyword}&start={page * 10}"
        browser.get(final_url) 

        #browser.save_screenshot("screenshot.png")
        soup = BeautifulSoup(browser.page_source, "html.parser")
        job_list = soup.find("ul", class_="jobsearch-ResultsList")
        jobs = job_list.find_all("li", recursive=False)
        for job in jobs:
            zone = job.find("div", class_="mosaic-zone")
            if zone == None:
                #h2 = job.find("h2", class_="jobTitle") & a = h2.find("a") => a = job.select("h2 a")
                #anchor = job.select("h2 a") # select() returns a list(<class 'bs4.element.ResultSet'>).
                anchor = job.select_one("h2 a") # select_one() returns a dictionary(<class 'bs4.element.Tag'>).
                #print(anchor.attrs)
                title = anchor["aria-label"] #<class 'bs4.element.Tag'>
                link = anchor["href"]
                company = job.find("span", class_="companyName").string
                location = job.find("div", class_="companyLocation").string
                #print(f"psition: {title}\nlink: {link}\ncompany: {company}\nlocation: {location}")
                #job_type = job.find("div.salaryOnly div.attribute_snippet")
                #print(job_type.string)
                try:
                    job_type = job.select_one("div.salaryOnly > div:nth-last-of-type(1) > div.attribute_snippet").text
                    if job_type.startswith("월급"):
                        job_type = ""
                except:
                    job_type = ""
                """
                # Dictionary version
                job_data = {
                    "title": title.replace(",", " "),
                    "company": company.replace(",", " "),
                    "job_type": job_type,
                    "location": location.replace(",", " "),
                    "link": f"https://kr.indeed.com{link}"
                }
                """
                # Dictionary > DataFrame() version
                job_data = {
                    "title": title,
                    "company": company,
                    "job_type": job_type,
                    "location": location,
                    "link": f"https://kr.indeed.com{link}"
                }
                # List > file.write() version
                #job_data = [title.replace(",", " "), company.replace(",", " "), job_type, location.replace(",", " "), f"https://kr.indeed.com{link}"]
                # List > DataFrame() version
                #job_data = [title.replace(",", " "), company.replace(",", " "), job_type, location.replace(",", " "), f"https://kr.indeed.com{link}"]
                results.append(job_data)
    
    browser.close()
    """
    for result in results:
        print(f"psition: {result['title']}\nlink: {result['link']}\ncompany: {result['company']}\nlocation: {result['location']}")
        print("----------------------------------------")    
    """
    return results